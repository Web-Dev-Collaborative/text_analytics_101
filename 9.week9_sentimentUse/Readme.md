## Gist : Understand and perform K-L divergence

Please refer [handout](9.week9_sentimentUse/handout.pdf) for objectives

### Main Tasks performed:

1. K-L divergence in Python

2. Understand how one probability distribution(PD) of doc 'i' is different from the reference PD of 'a'

For more implementation details refer [notebook](9.week9_sentimentUse/code/practical.ipynb) and [report](9.week9_sentimentUse/report.pdf)